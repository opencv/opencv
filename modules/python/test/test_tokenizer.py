#!/usr/bin/env python

'''
Test for Tokenizer Python bindings
'''

from __future__ import print_function

import cv2 as cv
import sys
import os

from tests_common import NewOpenCVTests

def _tf(filename=""):
    base = (os.environ.get("OPENCV_DNN_TEST_DATA_PATH")
            or os.environ.get("OPENCV_TEST_DATA_PATH")
            or os.getcwd())
    return os.path.join(base, "dnn", "llm", filename)

class TokenizerBindingTest(NewOpenCVTests):
    def test_tokenizer_binding(self):
        try:
            tokenizer = cv.dnn.Tokenizer
            print("Tokenizer binding is available.")
            gpt2_model = _tf("gpt2/config.json")
            tokenizer = cv.dnn.Tokenizer.load(gpt2_model)
            print("Tokenizer loaded from:", gpt2_model)
        except AttributeError:
            self.fail("Tokenizer binding is NOT available.")

    def test_tokenizer_gpt2(self):
        tok = cv.dnn.Tokenizer.load((_tf("gpt2/config.json")))
        ids = tok.encode("hello world")
        print(ids)
        txt = tok.decode(ids)
        self.assertEqual(txt, "hello world")

    def test_tokenizer_gpt4(self):
        tok = cv.dnn.Tokenizer.load(_tf("gpt4/config.json"))
        tokens = tok.encode("hello world")
        # expects {15339, 1917}
        self.assertEqual(list(tokens), [15339, 1917])
        sent = tok.decode([15339, 1917])
        self.assertEqual(sent, "hello world")

    def test_with_hf_tiktoken(self):
        from tiktoken import encoding_for_model
        from transformers import AutoTokenizer

        tik_tokenizer = encoding_for_model("gpt2")
        hf_tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2", use_fast=True)
        cv_tokenizer = cv.dnn.Tokenizer.load(_tf("gpt2/config.json"))

        sentences = {
            "Spanish": "Â¡Hola, mundo! Â¿CÃ³mo estÃ¡s?",
            "Chinese": "ä½ å¥½ï¼Œä¸–ç•Œï¼ä»Šå¤©å¤©æ°”ä¸é”™ã€‚",
            "Japanese": "ã“ã‚“ã«ã¡ã¯ä¸–ç•Œã€‚ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚",
            "Arabic": "Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…! ÙƒÙŠÙ Ø§Ù„Ø­Ø§Ù„ØŸ",
            "Hindi": "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾! à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?",
            "Russian": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, Ğ¼Ğ¸Ñ€! ĞšĞ°Ğº Ğ´ĞµĞ»Ğ°?",
            "Korean": "ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„! ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì¢‹ì•„ìš”.",
            "Thai": "à¸ªà¸§à¸±à¸ªà¸”à¸µà¹‚à¸¥à¸ à¸§à¸±à¸™à¸™à¸µà¹‰à¸­à¸²à¸à¸²à¸¨à¸”à¸µ",
            "Greek": "Î“ÎµÎ¹Î± ÏƒÎ¿Ï… ÎºÏŒÏƒÎ¼Îµ! Î¤Î¹ ÎºÎ¬Î½ÎµÎ¹Ï‚;",
            "Hebrew": "×©×œ×•× ×¢×•×œ×! ××” ×©×œ×•××š?",
            "Emojis": "ğŸ‘©ğŸ½â€ğŸ’»âœ¨ğŸš€ â€” coding time!",
            "Vietnamese": "Xin chÃ o tháº¿ giá»›i! HÃ´m nay trá»i Ä‘áº¹p.",
            "Turkish": "Merhaba dÃ¼nya! NasÄ±lsÄ±n?",
        }

        for i, (name, text) in enumerate(sentences.items()):
            cv_ids = cv_tokenizer.encode(text)
            tik_ids = tik_tokenizer.encode(text)
            hf_ids = hf_tokenizer.encode(text)

            print(f"--- {name} ---")
            print("text: ", text)
            print(f"cv token count: {len(cv_ids)} | tiktoken token count {len(tik_ids)} | hf token count {len(hf_ids)}")
            if i == 0: # First iteration output the types returned by encode for all 3 Tokenizer libraries
                print(f"type of cv ids [{type(cv_ids)}] | type of tiktoken ids [{type(tik_ids)}] | type of hf ids [{type(hf_ids)}]")
            self.assertEqual(cv_ids.tolist(), tik_ids, hf_ids)
            
if __name__ == '__main__':
    NewOpenCVTests.bootstrap()